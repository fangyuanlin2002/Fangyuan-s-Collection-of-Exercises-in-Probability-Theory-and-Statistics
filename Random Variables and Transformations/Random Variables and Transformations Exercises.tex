\documentclass[../main]{subfiles}
\begin{document}
\chapter{Random Variables and Transformations}
\section{Fundamentals}
\subsection{PDF Transformation Law for Monotone Transformations}
\begin{bbox}{2.1.5}
    Let $X$ have pdf $f_X(x)$ and let $Y=g(X)$, where $g$ is a monotone function. Suppose that $f_X(x)$ is continuous on $\mathcal X$ and that $g^{-1}$ has a continuous derivative on $\mathcal Y$. Then the pdf of $Y$ is given by 
    \[
    f_Y(y) = \begin{cases}
        f_X(g^{-1}(y))|\frac{\dd}{\dd y}g^{-1}(y)|\quad \text{if $y=g(x) \quad \text{for some $x\in \mathcal X$}\}$}\\
        0 \quad \text{otherwise}
    \end{cases}
    \]
\end{bbox}
\begin{pbox}{Square Transformation}
    Suppose $X$ is a continuous random variable. For $y>0$, the cdf of $Y=X^2$ is 
    \[
    F_Y(y) = \mathbb P(Y\le y) = \mathbb P(X^2 \le y) = \mathbb P(-\sqrt{y}\le X \le \sqrt{y}).
    \]
    By continuity, this is equal to 
    \[
    F_X(\sqrt{y}) - F_X(-\sqrt{y})
    \]
    whence differentiating gives us the pdf:
    \[
    f_Y(y) = \frac{1}{2\sqrt{y}}f_X(\sqrt{y})+\frac{1}{2\sqrt{y}}f_X(-\sqrt{y})
    \]
\end{pbox}
\subsubsection{Normal-chi squared relationship}
Let $X$ have the standard normal distribution,
\begin{bbox}{General pdf Transformation Law}
    Let $X$ have pdf $f_X(x)$ and let $Y=g(X)$. Suppose there exists a partition $\{A_i\}$ of $\mathcal X$ such that $\mathbb P(X\in A_0) = 0$ and $f_X(x)$ is continuous on each $A_i$. Furthermore, suppose there exist functions $\{g_i\}$ on $A_i$ such that 
    \begin{enumerate}
        \item $g(x) = g_i(x)$ on $A_i$
        \item $g_i(x)$ is monotone on $A_i$.
        \item The set $\mathcal Y = \{y: y=g_i(x) \quad  \text{for some $x\in A_i$}\}$ is the same for each $i=1,\dots,k$.
        \item $g_i^{-1}(y)$ has a continuous derivative on $\mathcal Y$
    \end{enumerate}
    Then the pdf of $Y$ is given by 
    \[
    f_Y(y) = \sum_{i=1}^k f_X(g_i^{-1}(y)|\frac{\dd}{\dd y}g_i^{-1}(y))
    \]
\end{bbox}


\section{Core Competency Exam Questions}
\subsection{Median Minimizes the Absolute Error}
\begin{bbox}{Median Minimizes the Absolute/$L^1$ Error}
    Let $X$ be a random variable. Show that the median of $X$ is the constant $a$ that minimizes $\mathbb E |X-a|$.
\end{bbox}
\begin{solution}
    \begin{enumerate}
        % \item By definition, the median $m$ of $X$ is a number such that 
        % \[
        % \Pr[X \geq m] \geq \frac{1}{2}, \quad \Pr[X\leq m] \geq \frac{1}{2}.
        % \]
        \item To make progress, we need to write something down. Let $f$ be the probability density function corresponding to $X$.
        \[
        E |X-a| = \int_{-\infty}^\infty |x-a| f(x)\dd x
        \]
        \item By linearity,
        \[
        E |X-a| = \int_{-\infty}^\infty |x-a| f(x)\dd x = \int_{-\infty}^a (a-x) f(x)  \dd x + \int_{a}^\infty (x-a) f(x) \dd x
        \]
        \item Differentiating with respect to $a$ and set the derivative to zero, we get that
        \[
        F(a) + af(a) - af(a) - af(a) - (\frac{\dd}{\dd a} aF(\infty) - aF(a)) = 0
        \]
        \item Simplify to get 
        \[
        F(a) - a f(a) - 1 + af(a) + F(a) = 0 \implies F(a) = \frac{1}{2}
        \]
        \item By definition, the minimizer $a$ is the median of $X$.
    \end{enumerate}
\end{solution}

\subsection{A Tight Bound of Variance of Bounded Random Variables}
\begin{bbox}{A Tight Bound of Variance of Bounded Random Variables}
Let $X$ be a random variable taking values in the interval $[0,1]$. 
    \begin{itemize}
    \item Show that the $\Var X \leq \frac{1}{4}$.
    \item Show that this bound is tight by finding a $X$ that achieves this bound.
\end{itemize}
\end{bbox}
\begin{solution}
    I provide two approaches to solve the first part. 
    \begin{enumerate}
        \item The first approach starts by noting the fact that $X^2 \leq X$ on $[0,1]$. Then we have that 
        \[
        \mathbb E [X^2] \leq \mathbb E [X].
        \]
        \item Then it's natural for us to consider the decomposition of variance 
        \[
        \Var X = \mathbb E [X^2] - \mathbb E[X]^2 \leq \mathbb E[X] - \mathbb E [X^2]. 
        \]
        \item Applying calculus to maximize $\mu - \mu^2$ on $[0,1]$, we get 
        \[
        \frac{\dd}{\dd \mu} (\mu - \mu^2) = 1-2\mu = 0 \quad \implies \mu = \frac{1}{2}. \quad \frac{\dd^2}{\dd\mu^2}(\mu-\mu^2) = -2 < 0.
        \]
        \item Finally, $\Var X \leq \frac{1}{2} - \frac{1}{2}^2 = \frac{1}{4}$. 
    \end{enumerate}
    \begin{enumerate}
        \item The second approach uses the fact that the expectation is the single constant-predictor that minimizes the mean square error. In particular, it's at least as good as the constant-predictor $\frac{1}{2}$:
        \[
        \mathbb E \left[(X-\mu)^2\right] \leq \mathbb E\left[(X-\frac{1}{2})^2\right].
        \]
        \item The maximum distance between $X$ and $\frac{1}{2}$ is $\frac{1}{2}$ since $X\in [0,1]$. Therefore,
        \[
        \mathbb E\left[(X-\frac{1}{2})^2\right] \leq \frac{1}{4}.
        \]
    \end{enumerate}
    For the second part of the problem, it was immediate for me to think of the random variable 
    \[
    X = \begin{cases}
        0 \quad \text{w.p. $\frac{1}{2}$} \\
        1 \quad \text{w.p. $1$} 
    \end{cases}.
    \]
    
    
\end{solution}
\begin{bbox}{{2018 Summer Practice, \#14}}
    Suppose $f: [0,\infty) \to \mathbb R$ is a function such that $f(x+y) = f(x)f(y)$.
    \begin{itemize}
        \item Show that $f(x)\geq 0$ for all real $x \geq 0$.
        \item Show that $f(0) \in \{0,1\}$.
        \item Show that for any nonnegative rational number $r$ one has $f(r)=c^r$, where $c\in[0,\infty)$.
        \item Suppose $X$ is a non-negative random variable such that 
        \[
        P(X > s+t) = P(X>s) P(X>t)
        \] for every $s,t \geq 0$. If $X$ has a continuous distribution function, name the distribution of $X$.
    \end{itemize}
\end{bbox}
\begin{solution}
    First of all, this just looks like the exponential function.
    \begin{itemize}
        \item \[
        f(x) = f(\frac{x}{2} + \frac{x}{2}) = f(\frac{x}{2})^2 \geq 0
        \]
        \item \[
        f(0) = f(0)^2 \implies f(0) \in \{0,1\}
        \
        \]
        \item First of all, we can use proof by induction to show that $f(n) = c^n$ for $n\in \mathbb N$. by letting $c = f(1)$.
        \newline
        $r$ is nonnegative rational number so $r=\frac{p}{q}$ where $p,q$ are positive integers. Note \[
        f(1) = f\left(\frac{1}{q} * q\right) =f\left(\frac{1}{q}\right)^q \implies f\left(\frac{1}{q}\right) = c^{\frac{1}{q}} 
        \]
        This then implies \[
        f\left(\frac{p}{q}\right) = c^{\frac{p}{q}}
        \]
        Comment: Thank god I'm at least able to solve this problem single-handedly.
        \item The set of rational numbers is dense in $\mathbb R$. I hope the reader understands what this means.
        \item This is apparently the exponential distribution.
        \newline
        Let $f(x) = 1 - F(x)$ where $F(x)$ is the cdf of $X$. Then 
        \[
        f(s+t) = f(s)f(t)
        \]
        By the previous parts, $f(\cdot)$ is the exponential function, which characterizes the exponential function.
    \end{itemize}
\end{solution}
\begin{bbox}{2018 September \#2}
    We consider balls of random radius $R$.
    \begin{enumerate}
        \item Suppose that $R$ is uniformly distributed on $[1,10]$. Find the probability density function of the volume $V$ of a ball (Recall that $V=\frac{4}{3}\pi R^3)$
    \end{enumerate}
\end{bbox}



\end{document}