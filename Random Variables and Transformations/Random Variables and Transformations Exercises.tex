\documentclass[../main]{subfiles}
\begin{document}
\chapter{Random Variables and Transformations}
\section{Fundamentals}
\subsection{PDF Transformation Law for Monotone Transformations}

\begin{bbox}{Theorem 2.1.3}
    Let $X$ have cdf $F_X(x)$, let $Y = g(X)$,
    \begin{itemize}
        \item If $g$ is an increasing function, then $F_Y(y) = F_X(g^{-1}(y))$ for $y\in\mathcal Y$.
        \item If $g$ is a decreasing function and $X$ is a continuous random variable, $F_Y(y) = 1-F_X(g^{-1}(y))$
    \end{itemize}
\end{bbox}
\begin{pbox}{Uniform-Exponential Relationship Part 1}
    Suppose $X\sim f_X(x) = 1$ for $0<x<1$. Let $Y=-\log(X)$.
    \[
    \frac{\dd}{\dd x}g(x) = -\frac{1}{x}
    \]
    which is decreasing on $[0,1]$. $-\log(x)$ ranges from $0$ to $\infty$ on this interval. Note that 
    \[
    g^{-1}(y) = e^{-y}.
    \]
    Therefore, 
    \[
    F_Y(y) = 1- F_X(e^{-y}) = 1-e^{-y}
    \]
    This is the exponential distribution.
\end{pbox}
\begin{bbox}{Theorem 2.1.5}
    Let $X$ have pdf $f_X(x)$ and let $Y=g(X)$, where $g$ is a monotone function. Suppose that $f_X(x)$ is continuous on $\mathcal X$ and that $g^{-1}$ has a continuous derivative on $\mathcal Y$. Then the pdf of $Y$ is given by 
    \[
    f_Y(y) = \begin{cases}
        f_X(g^{-1}(y))|\frac{\dd}{\dd y}g^{-1}(y)|\quad \text{if $y=g(x) \quad \text{for some $x\in \mathcal X$}\}$}\\
        0 \quad \text{otherwise}
    \end{cases}
    \]
\end{bbox}
\begin{pbox}{Square Transformation}
    Suppose $X$ is a continuous random variable. For $y>0$, the cdf of $Y=X^2$ is 
    \[
    F_Y(y) = \mathbb P(Y\le y) = \mathbb P(X^2 \le y) = \mathbb P(-\sqrt{y}\le X \le \sqrt{y}).
    \]
    By continuity, this is equal to 
    \[
    F_X(\sqrt{y}) - F_X(-\sqrt{y})
    \]
    whence differentiating gives us the pdf:
    \[
    f_Y(y) = \frac{1}{2\sqrt{y}}f_X(\sqrt{y})+\frac{1}{2\sqrt{y}}f_X(-\sqrt{y})
    \]
\end{pbox}
\subsubsection{Normal-chi squared relationship}
Let $X$ have the standard normal distribution,
\begin{bbox}{General pdf Transformation Law}
    Let $X$ have pdf $f_X(x)$ and let $Y=g(X)$. Suppose there exists a partition $\{A_i\}$ of $\mathcal X$ such that $\mathbb P(X\in A_0) = 0$ and $f_X(x)$ is continuous on each $A_i$. Furthermore, suppose there exist functions $\{g_i\}$ on $A_i$ such that 
    \begin{enumerate}
        \item $g(x) = g_i(x)$ on $A_i$
        \item $g_i(x)$ is monotone on $A_i$.
        \item The set $\mathcal Y = \{y: y=g_i(x) \quad  \text{for some $x\in A_i$}\}$ is the same for each $i=1,\dots,k$.
        \item $g_i^{-1}(y)$ has a continuous derivative on $\mathcal Y$
    \end{enumerate}
    Then the pdf of $Y$ is given by 
    \[
    f_Y(y) = \sum_{i=1}^k f_X(g_i^{-1}(y)|\frac{\dd}{\dd y}g_i^{-1}(y))
    \]
\end{bbox}

\subsection{Expected Values}
\begin{pbox}{Uniform-Exponential Relationship-II}
    Let $X$ have a uniform (0,1) distribution. Define a new random variable $g(X) = -\log(X)$. Then
    \[
    \mathbb E g(X) = \mathbb E(-\log X) = \int_{0}^{1}-\log x \dd x= x - x\log x|_{0}^{1} = 1
    \]
    But we also saw that $Y=-\log X$ has cdf $1-e^{-y}$
\end{pbox}
\begin{pbox}{Cauchy Distribution does not have finite expected value}
    Let $X$ be a Cauchy random variable, i.e. 
    \[
    f_X(x) = \frac{1}{\pi}\frac{1}{1+x^2}, \quad -\infty < x < \infty
    \]
    \begin{itemize}
        \item \[
        \int_{0}^{\infty}\frac{1}{1+x^2}\dd x =\arctan(\infty)-\arctan(0) = \frac{\pi}{2}
        \]
        so it's straighforward to check that this is a probability distribution.
        \item However, the $\mathbb E|X| = \infty$.
        \begin{align*}
            &\mathbb E|X| = \int_{-\infty}^\infty \frac{|x|}{\pi}\frac{1}{1+x^2}\dd x\\
            &=\frac{2}{\pi}\int_0^\infty \frac{x}{1+x^2}\dd x
        \end{align*}
        For any positive number $M$,
        \[
        \int_{0}^{M} \frac{x}{1+x^{2}} \, \mathrm{d}x 
        = \left. \frac{\log(1+x^{2})}{2} \right|_{0}^{M} = \frac{\log(1+M^2)}{2}
        \]
        Then 
        \[
        \mathbb E|X| = \lim_{M\to\infty}\frac{1}{\pi}\log(1+M^2) = \infty.
        \]
    \end{itemize}
\end{pbox}
\subsection{Moment Generating Function}
\begin{gbox}{Moment Generating Function}
    The moment generating function of the random variable $X$ is 
    \[
    M_X(t) = \mathbb E \left[e^{Xt}\right]
    \]
    provided that the expectation exists for $t$ in some neighborhood of $0$.
\end{gbox}
\begin{bbox}{MGF generates the moments}
    \[
    \frac{\dd^n}{\dd t^n} M_X(t)|_{t=0} = \mathbb E[X^n]
    \]
\end{bbox}
\subsubsection{Gamma Moment Generating Function}
\begin{pbox}{Gamma Moment Generating Function}
    Consider the gamma pdf 
    \[
    f(x) = \frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{-x/\beta}, \quad 0 < x <\infty, \alpha>0, \beta > 0
    \]
    where $\Gamma(\alpha)$ denotes the gamma function evaluated at $\alpha$. The mgf is then 
    \[
    M_X(t) = \frac{1}{\Gamma(\alpha)\beta^\alpha}\int_{0}^\infty e^{tx}x^{\alpha-1}e^{-x/\beta}\dd x = \frac{1}{\Gamma(\alpha)\beta^\alpha}\int_{0}^\infty x^{\alpha-1}e^{-x/(\frac{\beta}{1-\beta t})}\dd x
    \]
    Note that the integrand is the kernel (i.e. the part disregarding the normalizing constant) of another gamma pdf. Therefore, the integral, assuming it exists, evaluates to $\Gamma(\alpha)(\frac{\beta}{1-\beta t})^\alpha$. Then
    \[
    M_X(t) = \frac{1}{\Gamma(\alpha)\beta^\alpha}\Gamma(\alpha)\left(\frac{\beta}{1-\beta t}\right)^\alpha = \left(\frac{1}{1-\beta t}\right)^\alpha,
    \] if $t < 1/\beta$. Otherwise, the integral diverges so to the mgf doesn't exist. As an application of the mgf, the mean of the gamma distribution can be found by
    \[
    \mathbb E [X] = \left. \frac{\dd}{\dd t}M_X(t)\right|_{t=0} = -\alpha (1-\beta t)^{-\alpha-1} \cdot (-\beta) = \alpha \beta.
    \]
\end{pbox}
\section{Core Competency Exam Questions}

\subsection{Problem 3}
\begin{bbox}{3: 2018 Summer Practice \#9}
    Suppose $X_1, X_2$ are i.i.d. random variables from a distribution $F$ with mean $0$ and variance $1$.
    \begin{itemize}
        \item If $F=N(0,1)$, show that 
        \[
        \frac{X_1 + X_2}{\sqrt{2}}\overset{d}{=} X_1
        \]
        \item If \[
        \frac{X_1 + X_2}{\sqrt{2}}\overset{d}{=} X_1,
        \]
        show that $F=N(0,1)$.
    \end{itemize}
\end{bbox}
\begin{solution}
    For part $a$, we know that $X_1,X_2$ are independent standard normal variables. Therefore, their sum follows that $N(0+0, 1+1)$ distribution. By scaling the sum by $\sqrt{2}$, we get an $N(0,1)$ random variable.
    \newline
    For the other direction, we can use the fact that MGF characterizes distributions. 
    \[
        \frac{X_1 + X_2}{\sqrt{2}}\overset{d}{=} X_1,
        \] means that 
        \[
        M_{\frac{X_1+X_2}{\sqrt{2}}}(t) = M_{X_1}(t).
        \]
        By independence, we know that 
        \[
        M_{\frac{X_1+X_2}{\sqrt{2}}} = M_{X_1}(\frac{1}{\sqrt{2}}t)M_{X_2}(\frac{1}{\sqrt{2}}t) = M_{X_1}(\sqrt{2}t)^2 = M_{X_1} 
        \]
        Next, the trick is to take log of the above functional equation!!
\end{solution}
\subsection{Problem 4}
\begin{bbox}{4: 2018 Summer Practice \#14}
    Suppose $f:[0,\infty)\to \mathbb R$ is a function such that $f(x+y)=f(x)f(y)$. \begin{itemize}
        \item Show that $f9x)\ge 0$ for all real $x\ge 0$.
        \item Show that $f(0)\in \{0,1\}$.
        \item Show that for any nonnegative rational number $r$ one has $f(r) = c^r$ where $c\in [0,\infty)$
        \item ...
    \end{itemize}
\end{bbox}
\begin{solution}
    An easy problem in my opinion.
\end{solution}
\subsection{Problem 5}
\begin{bbox}{5: 2018 Summer \#15}
    Suppose $X$ is a random variable taking values in $[0,1]$. \begin{itemize}
        \item Show that $\Var(X)\le \frac{1}{4}$.
        \item Find a random variable for which equality holds.
    \end{itemize}
\end{bbox}
\begin{solution}
    $1/4$ is a special number and is related to $\frac{1}{2}$. First note that $\mathbb EX$ is the best constant estimator in the MSE sense, so 
    \[
    \mathbb E[(X-\mathbb EX)^2] \le \mathbb E[(X-\frac{1}{2})^2]=\mathbb E[X^2]-\mathbb E[X] + \frac{1}{4} \le \frac{1}{4}
    \]
    since $X^2 \le X$. The Bernoulli rv that takes 0 with probability $0.5$ and 1 with probability $0.5$ achieves equality for this variance bound.
\end{solution}
\subsection{Problem 7}
\begin{bbox}{7: 2018 September \# 4}
    \begin{itemize}
        \item Let $X$ be a random variable and $a\in\mathbb R$. Show that (using Markov's inequality or otherwise):
        \[
        \mathbb P[X\ge a] \le \inf_{s\ge 0} e^{-sa}\mathbb E[e^{sX}].
        \]
        \item Let $N$ be a Poisson random variable with parameter $\lambda > 0$, i.e. 
        \[
        \mathbb P[N=n] = e^{-\lambda}\frac{\lambda^n}{n},\quad n\ge 0
        \]
        Show that $\mathbb E[e^{sN} = e^{\lambda(e^s -1)}]$ for all $s\in \mathbb R$.
        \item Let $N$ be as in ii and let $m\ge \lambda$ be an integer. Use i and ii to show that 
        \[
        P(N\ge m) \le (\frac{\lambda}{m})^m e^{m-\lambda}
        \]
    \end{itemize}
\end{bbox}
\
\begin{solution}


\begin{itemize}
    \item By the Markov inequality, 
    \[
    P(X\ge a) = P(e^{sX} \le e^{sa}) \leq e^{-sa}\mathbb E[e^{sX}]
    \]
    for any positive $s$. Therefore,
    \[
        \mathbb P(X\ge a) \le e^{-sa}\mathbb E[e^{sX}]
    \]
    \item 
    \begin{align*}
        \mathbb E[e^{sN}] &= \sum_{n=0}^\infty e^{-sn}\,e^{-\lambda}\frac{\lambda^n}{n!}\\
        &= e^{-\lambda}\sum_{n=0}^\infty e^{-sn}\frac{\lambda^n}{n!}\\
        &= e^{-\lambda}\sum_{n=0}^\infty \frac{(e^{-s}\lambda)^n}{n!}\\
        &= e^{-\lambda}e^{e^{-s}\lambda}\\
        &= e^{\lambda(e^s - 1)}
    \end{align*}
    \item 
    \[
    P(N \ge m) \le \inf_{s\le 0} e^{-sm}e^{\lambda(e^s -1)}
    \]
    Let's find the minimizing $s$.
    Letting the derivative be $0$, we get 
    \[
        -me^{-sm}e^{\lambda(e^s -1)}+e^{-sm} \lambda \, e^{s}e^{\lambda(e^s - 1)} = 0
    \]
    This simplifies to 
    \[
    \lambda e^s-m = 0 \iff e^{-s} = \frac{\lambda}{m}
    \]
    Therefore $\inf_{s\ge 0} e^{-sa}\mathbb E [e^{sX}] = \left(\frac{\lambda}{m}\right)^m e^{\lambda (\frac{m}{\lambda} -1)} = \left(\frac{\lambda}{m}\right)^m e^{m-\lambda}$.
\end{itemize}
\end{solution}
\subsection{Problem 8}
\begin{bbox}{2019 May \# 4}
    We model the lifetime of a device as a random variable $T\ge 0$ with cdf $F(t)$ and density $f(t)$. Suppose that $f(t)$ is continuous for $t\ge 0$ and define the intensity of failure as 
    \[
    \lambda(t) = \lim_{h\downarrow0} \frac{P(t\le T \le t+h|T\ge t)}{h}, \quad \text{for $t\ge 0$.}
    \]
    \begin{itemize}
        \item Express $\lambda(t)$ through $f(t)$ and $F(t)$.
        \item Compute the intensity of failure when $T\sim \exp(\alpha),\alpha>0$.
        \item Show that $F(t) = 1-\exp(-\int_{0}^{t}\lambda(s)\dd s)$ for $t\ge 0$.
        \item Determine $F(t)$ and $f(t)$ in the case that $\lambda(t) = \alpha t^{\gamma}$ for some $\alpha>0$ and $\gamma > 0$.
    \end{itemize}
\end{bbox}
\begin{solution}
    For part a, let's just rewrite the expression from the definition using $f$ and $F$. 
    \begin{align*}
        P(t\le T\le t+h | T\ge t) &= \frac{F(t+h)-F(t)}{1-F(t)}\\
        \lim_{h\downarrow 0} \frac{F(t+h)-F(t)}{h}&= f(t)
    \end{align*}
    Therefore 
    \[
    \lambda(t) = \frac{f(t)}{1-F(t) }
    \]
    For part b, 
    \begin{align*}
        \lambda(t) &= \frac{f(t)}{1-F(t)}\\
        &= \frac{\alpha e^{-\alpha t}}{e^{-\alpha t}} \mathbf{1}_{t\ge 0}\\
        &= \alpha \mathbf{1}_{t\ge 0}
    \end{align*}
    For part c, first of all
    \[
    \int_{0}^t \lambda(s)\dd s = \int_{0}^{t} \frac{f(s)}{1-F(s)}\dd s.
    \]
    Let $y=F(t)$, then $\frac{\dd y}{\dd t} = f(t)$ and thus $\dd y = f(t)\dd t$.\\
    Now the above integral becomes 
    \[
    \int_{F(0)}^{F(t)}\frac{1}{1-y}\dd y = \left. \ln (1-y) \right |_{F(t)}^{F(0)} = -\ln (1-F(t))
    \]
    Now 
    \[
    1- \exp\{\ln(1-F(t))\} = 1 -1+F(t) = F(t).
    \]
    For part d, we should go from part c and compute $F(t)$. 
    \begin{align*}
        F(t) &= 1-\exp\{-\int_{0}^t\lambda(s)\dd s\}\\
        \int_{0}^t\alpha s^{\gamma} \dd s &= \frac{\alpha}{\gamma + 1}t^{\gamma + 1}
    \end{align*}
    Then 
    \[
    F(t) = 1 - \exp\{-\frac{\alpha}{\gamma + 1} t^{\gamma + 1}\}
    \]
    Then by differentiating $F(t)$, we get 
    \[
    f(t) = -\exp\{-\frac{\alpha}{\gamma + 1}t^{\gamma + 1}\} (-\alpha t^\gamma)
    \]
\end{solution}
\subsection{Problem 9}
\begin{bbox}{2019 May \#5}
    You are working as a TA in the help room for a duration $t$. The number of students arriving during that period is Poisson distributed with parameter $t\lambda$. For each student, the time $T$ to answer their questions is exponentially distributed with parameter $\alpha$ and this time is independent of all other students. Prove that the distribution of the number $X$ of students that arrive while you are busy with one fixed (randomly chosen) student is geometric with some parameter $p$ and determine $p$ in terms of $\alpha$ and $\lambda$.
    \newline
    Hint: The formula
    \[
    \int_{0}^\infty s^k e^{-s} \dd s = s!
    \] for $k=0,1,2,\dots$ can be used without proof.
\end{bbox}
\begin{solution}
    This problem is all about computing integral. 
    \begin{itemize}
        \item First of all, we know
        \[
        \mathbb P(X=k|T=t) \sim \text{Poisson}(\lambda t)
        \]
        \item Now
        \[
        P(X=n)=\int_{0}^\infty P(X=n|T=t)f(t) \dd t
        \]
        \item 
        \begin{align*}
            P(X=n)&=\int_{0}^\infty \frac{e^{-t\lambda}(t\lambda)^n}{n!} \alpha e^{-\alpha t} \dd t\\
            &=\frac{\alpha}{n!}\frac{1}{\alpha+\lambda}\int_{0}^\infty e^{-s}\left(\frac{\lambda}{\alpha+\lambda}s\right)^n \dd s\\
            &= \frac{\alpha \lambda^n}{(\alpha+\lambda)^{n+1}}
        \end{align*}
        This is a geometric random variable with parameter $p = \frac{\lambda}{\alpha+\lambda}$
    \end{itemize}
\end{solution}
\subsection{Problem 10}
\begin{bbox}{2019 May \# 7}
    Suppose you have $n$ red balls and one blue ball. We will do two experiments. In the first experiment, you first drop $n$ red balls uniformly on the interval $[0,1]$, independent of each other. Having done this, now you drop the blue ball uniformly in the interval, independent of all previous ball drops. Let $X$ denote the number of red balls to the left of the blue ball. Find $P(X=k)$, for $k=0,...,n$.
    \newline
    In the second, experiment, you drop all the $(n+1)$ balls uniformly on $[0,1]$, independent of each other. Let $Y$ denote the number of red balls to the left of the blue ball as before. Find $\mathbb P(Y=k).$
\end{bbox}
\begin{solution}
    Let $R_1,\dots, R_n$ be the positions of the red balls, which are i.i.d. Uniform(0,1). Let $B$ be the position of the blue ball. Let 
    \[
    X = \mathbf{1}_{B\le U_1} + \dots + \mathbf{1}_{B\le U_n}.
    \]
    Given $B=b$, $X$ is a binomial(b, n) random variables. Then
    \[
    P(X=k) = \int_{0}^1 \binom{n}{k}b^{k}(1-b)^{n-k}\dd b = \binom{n}{k} \beta(k+1, n-k+1)
    \]
    This simplifies to
    \[
    \frac{n!}{k!(n-k)!}\frac{k!(n-k)!}{(n+1)!} = \frac{1}{n+1}.
    \]
    This is my original and somewhat canonical idea, which differs from the official solution:
    \[
    P(X=k)=\mathbb E_{U_1,\dots,U_n}[P(X=k|U_1,\dots, U_n)] = \mathbb E[U_{(k+1)}-U_{(k)}] = \frac{k+1}{n+1}-\frac{k}{n+1} = \frac{1}{n+1}.
    \]
    For this to work, we need to know that the order statistics $U_{(k)} \sim \text{Beta}(k, n-k+1)$.
\end{solution}
\subsection{Problem 11}
\begin{bbox}
    Suppose that $X$ is a non-negative random variable. Show that
\end{bbox}
\section{Supplementary Questions}
\subsection{Median Minimizes the Absolute Error}
\begin{bbox}{Median Minimizes the Absolute/$L^1$ Error}
    Let $X$ be a random variable. Show that the median of $X$ is the constant $a$ that minimizes $\mathbb E |X-a|$.
\end{bbox}
\begin{solution}
    \begin{enumerate}
        % \item By definition, the median $m$ of $X$ is a number such that 
        % \[
        % \Pr[X \geq m] \geq \frac{1}{2}, \quad \Pr[X\leq m] \geq \frac{1}{2}.
        % \]
        \item To make progress, we need to write something down. Let $f$ be the probability density function corresponding to $X$.
        \[
        E |X-a| = \int_{-\infty}^\infty |x-a| f(x)\dd x
        \]
        \item By linearity,
        \[
        E |X-a| = \int_{-\infty}^\infty |x-a| f(x)\dd x = \int_{-\infty}^a (a-x) f(x)  \dd x + \int_{a}^\infty (x-a) f(x) \dd x
        \]
        \item Differentiating with respect to $a$ and set the derivative to zero, we get that
        \[
        F(a) + af(a) - af(a) - af(a) - (\frac{\dd}{\dd a} aF(\infty) - aF(a)) = 0
        \]
        \item Simplify to get 
        \[
        F(a) - a f(a) - 1 + af(a) + F(a) = 0 \implies F(a) = \frac{1}{2}
        \]
        \item By definition, the minimizer $a$ is the median of $X$.
    \end{enumerate}
\end{solution}

\subsection{A Tight Bound of Variance of Bounded Random Variables}
\begin{bbox}{A Tight Bound of Variance of Bounded Random Variables}
Let $X$ be a random variable taking values in the interval $[0,1]$. 
    \begin{itemize}
    \item Show that the $\Var X \leq \frac{1}{4}$.
    \item Show that this bound is tight by finding a $X$ that achieves this bound.
\end{itemize}
\end{bbox}
\begin{solution}
    I provide two approaches to solve the first part. 
    \begin{enumerate}
        \item The first approach starts by noting the fact that $X^2 \leq X$ on $[0,1]$. Then we have that 
        \[
        \mathbb E [X^2] \leq \mathbb E [X].
        \]
        \item Then it's natural for us to consider the decomposition of variance 
        \[
        \Var X = \mathbb E [X^2] - \mathbb E[X]^2 \leq \mathbb E[X] - \mathbb E [X^2]. 
        \]
        \item Applying calculus to maximize $\mu - \mu^2$ on $[0,1]$, we get 
        \[
        \frac{\dd}{\dd \mu} (\mu - \mu^2) = 1-2\mu = 0 \quad \implies \mu = \frac{1}{2}. \quad \frac{\dd^2}{\dd\mu^2}(\mu-\mu^2) = -2 < 0.
        \]
        \item Finally, $\Var X \leq \frac{1}{2} - \frac{1}{2}^2 = \frac{1}{4}$. 
    \end{enumerate}
    \begin{enumerate}
        \item The second approach uses the fact that the expectation is the single constant-predictor that minimizes the mean square error. In particular, it's at least as good as the constant-predictor $\frac{1}{2}$:
        \[
        \mathbb E \left[(X-\mu)^2\right] \leq \mathbb E\left[(X-\frac{1}{2})^2\right].
        \]
        \item The maximum distance between $X$ and $\frac{1}{2}$ is $\frac{1}{2}$ since $X\in [0,1]$. Therefore,
        \[
        \mathbb E\left[(X-\frac{1}{2})^2\right] \leq \frac{1}{4}.
        \]
    \end{enumerate}
    For the second part of the problem, it was immediate for me to think of the random variable 
    \[
    X = \begin{cases}
        0 \quad \text{w.p. $\frac{1}{2}$} \\
        1 \quad \text{w.p. $1$} 
    \end{cases}.
    \]
\end{solution}




\end{document}