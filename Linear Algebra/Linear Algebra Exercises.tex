\documentclass[../main]{subfiles}
\begin{document}
\chapter{Linear Algebra}
\section{Eigenvalue and Eigenvector}
\begin{bbox}{The eigenvalues of $A^2$}
    If $A$ has eigenvalues $\lambda_i$. Then the eigenvalues of $A^2$ are $\lambda_i^2$.
    \begin{proof}
        Well, my first intuition is to think about the diagonalization of $A$ and the result becomes clear.
        \newline
        A rigorous proof is also not hard: 
        \begin{enumerate}
            \item \begin{align*}
                A\vec v = \lambda \vec v &\implies A^2 \vec v = A \lambda\vec v = \lambda^2 \vec v.
            \end{align*}
        \item 
            The algebraic multiplicity of the eigenvalues $\lambda^2_i$ of $A^2$ is the same as the eigenvalues $\lambda_i$ of $A$:
            \[
            \det (A^2 - \lambda^2 I) = \det (A+\lambda I)\det (A-\lambda I).
            \]
            This means that
        \end{enumerate}
    \end{proof}
\end{bbox}

\section{Orthogonality}
Orthogonal Matrices have the following properties:
\begin{itemize}
    \item $U^{-1}=U^T$
    \item Rows and columns are orthogonal unit vectors.
    \item Preserves the inner product of vectors: $\langle x,y\rangle = \langle Ux, Uy\rangle$.
    \item Isometric: length/distance preserving.
    \item Rigit rotation, reflection, rotoreflection.
\end{itemize}

\section{Singular Value Decomposition}
The Principal Component Analysis PCA is a byproduct of SVD. Let $X$ denote the original data matrix. Let $X^*$ be the (column-)centered matrix. (In machine learning, each row represents a data point and each column represents a feature). Let $\hat X$ denote the centered, normalized matrix.
\begin{itemize}
    \item $X^{*T}X^*$ is the covariance matrix.
    \item $\hat X^T \hat X$ is the correlation matrix.
    \item $X^T X$ is the cross-product.
\end{itemize}
Consider an $n\times d$ matrix $A$
\begin{bbox}{Singular Value Decomposition}
    Let $A$ be an $n\times d$ matrix with singular vectors $v_1,\dots, v_r$ and corresponding singular values $\sigma_1,\dots, \sigma_r$. Then $u_i = \frac{1}{\sigma_i} Av_i$ are the left singular vectors and $A$ can be decomposed into a sum of rank one matrices as 
    \[
    A = \sum_{i=1}^r \sigma_i u_i v_i^T
    \]
    \begin{itemize}
        \item Compute $A^T A$. This is a symmetric and positive semi-definite matrix, so it has orthonormal eigenvectors $v_i$ and non-negative eigenvalues $\lambda_i$ by the Spectral theorem.
        \item The singular values are $\sigma_i = \sqrt{\lambda_i}$
        \item $V = [v_i]$ is the matrix of right singular values.
        \item Define $u_i = \frac{1}{\sigma_i}Av_i$
    \end{itemize}
\end{bbox}
\begin{pbox}{Intuition behind SVD}
    Consider the optimization problem:
    \[
    \max_{\|v\|=1} \|Av\| = \max_{\|v\|=1} v^T A^T Av
    \]
    This is a Rayleigh quotient of $A^T A$, so its maximum value corresponds to its largest eigenvalue!
    \begin{itemize}
        \item If we define $u_i = \frac{1}{\sigma_i}Av_i$, then we get $Av_i = \sigma_i u_i$ and $u_i$ are orthonomal vectors.
    \end{itemize}
    Let's verify the algorithm is correct:
    \[
    u_i^T A v_i = (\frac{1}{\sigma_i} Av_i)^TAv_j = \frac{\sigma_i^2}{\sigma_i} = \sigma_i
    \]
\end{pbox}
\begin{itemize}
    \item We can think of $A$ as three steps: rotation $V^T$, then horizontal/vertical scaling ($\Sigma$), lastly rotation $U$.
    \item Use \begin{verbatim}
        from scipy import linalg
        U, s, Vh = linalg.svd(X)
    \end{verbatim}
    to compute the SVD of matrix $X$.
    \item Any matrix can be quickly decomposed into SVD form.
    \item One important and obvious application of SVD is data compression.
\end{itemize}
\begin{pbox}{Apply SVD to a two by two matrix}
    Let $M = \begin{bmatrix}
        0 & 1\\
        -1 & 0
    \end{bmatrix}$, rotation by ninety degrees clockwise. Note that this matrix has complex eigenvalue of $-i$. Let's compute the SVD of it.
    \[
    SVD(M) = \begin{bmatrix}
        0 & 1\\
        1 & 0
    \end{bmatrix} \begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix}
    \begin{bmatrix}
        -1 & 0 \\
        0 & 1
    \end{bmatrix}
    \]
    
    \begin{itemize}
        \item $V^T$ is reflection about the $y$-axis / rotation by 180 degress about $y$-axis.
        \item $\Sigma$ is the trivial scaling.
        \item $U$ is rotation by 180 degress about the origin. (reflection about both the $x$ and $y$-axes).
        \item The rank $1$ approximation would be $\begin{bmatrix}
            0 & 0\\
            -1 & 0
        \end{bmatrix}$
    \end{itemize}
\end{pbox}
\section{Determinant}
\subsection{The Determinant is the Product of Eigenvalues}
\begin{proof}
    Let $A$ be a matrix with eigenvalues $\lambda_i$. The key idea of the proof uses the characteristic polynomial.
    \begin{enumerate}
        \item Consider the characteristic polynomial
        \[
        p(\lambda) = |\lambda I - A| = c_0 + c_1 \lambda + \dots + \lambda^n
        \]
        Note that the characteristic polynomial is monic.
        \item We can obtain $c_0$ by
        \[
        p(0) = c_0 = |0\cdot I - A| = (-1)^n \det A
        \]
        \item Note that the eigenvalues $\lambda_i$ are roots of the characteristic polynomial so 
        \[
        p(0) = \prod_i(0-\lambda_i) = (-1)^n \lambda_i
        \]
        \item Lastly, 
        \[
        c_0 = (-1)^n \prod_{i}\lambda_i  = (-1)^n \det A
        \]
        so 
        \[
        \det A = \prod_i \lambda_i
        \]
    \end{enumerate}
\end{proof}
\section{Trace}
\subsection{Trace is Equal to the Sum of Eigenvalues}
\begin{bbox}{Trace is Equal to the Sum of Eigenvalues}
    Let $A$ be an $n\times n$ matrix with eigenvalues $\lambda_i$. Show that 
    \[
    \Tr(A) = \sum_i \lambda_i
    \]
\end{bbox}
\begin{solution}
    \begin{enumerate}
        \item The proof is similar to that of "the determinant is product of eigenvalues," i.e. we work with the characteristic polynomial. \textcolor{red}{TO BE FILLED IN} 
    \end{enumerate}
\end{solution}
\subsection{An Inequality relating Trace and Determinant}
\begin{bbox}{{2018 Summer Practice Problem, \# 18}}
Suppose $\Sigma$ is a non-negative definite matrix of $n\times n$ real entries and real eigenvalues. Show that 
\[
\Tr(\Sigma^2) \geq n \cdot \det(\Sigma)^{2/n}.
\]
\end{bbox}
\begin{solution}
    \begin{enumerate}
        \item Let $\{\lambda_i\}$ be the eigenvalues of $\Sigma$. To make some progress, let's write the trace as 
        \[
        \Tr(\Sigma) = \sum_i \lambda_i^2        
        \]
        \item By the Arithmetic Mean - Geometric Mean inequality,
        \[
        \frac{\sum_{i=1}^n \lambda_i^2}{n} \geq \sqrt[n]{\prod_{i=1}^n \lambda_i^2} \implies \Tr(\Sigma^2) \geq n \det(\Sigma)^{\frac{2}{n}}
        \]
    \end{enumerate}
\end{solution}
\section{Core Competency Exam Questions}
\begin{bbox}{{2020 September Exam, \#8}}
    For every $n\geq 1$, let $A_n$ be an $n\times n$ symmetric matrix with non-negative entries. Let $R_n(i):= \sum_{j=1}^n A_n(i,j)$ denote the ith row/column sum of $A_n$. Assume that 
    \[
    \lim_{n\to \infty} \max_{1\leq i \leq n} |R_n(i) - 1| = 0.
    \]
    Let $\lambda_n \geq 0$ denote an eigenvalue with the largest absolute value, and let $\vec x = (x_1, \dots, x_n)$ denote its corresponding eigenvector. 
    \begin{itemize}
        \item Show that 
        \[
        \frac{1}{n} \sum_{i,j=1}^n A_n(i,j) \to 1
        \]
        \item Show that $\lambda_n |x_i| \leq \max_{1\leq j \leq n} |x_j| R_n(i)$.
        \item Using parts one and two, show that 
        \[
        \lambda_n \to 1.
        \]
    \end{itemize}
\end{bbox}
\begin{solution}
    For the first part, let's just write something down:
    \begin{enumerate}
        \item 
        \begin{align*}
        \frac{1}{n} \sum_{i,j=1}^n A_n(i,j) &= \frac{1}{n} \sum_{i=1}^n R_n(i)
    \end{align*}
        \item \begin{align*}
        \left|\frac{1}{n} \sum_{i,j=1}^n A_n(i,j) - 1\right| &= \left|\frac{1}{n} \sum_{i=1}^n R_n(i) - 1\right|\\
        &\leq \max_{1\leq i\leq n} |R_n(i) - 1| \to 0
    \end{align*}
    \end{enumerate}
    
    For the second part, 
    \begin{enumerate}
        \item By assumption, \begin{align*}
        A_n \vec x &= \lambda_n \vec x, \quad \lambda_n x_i = \sum_{j=1}^n A_n(i,j)x_j\\
        \lambda_n |x_i|&\leq \sum_{j=1}^n A_n(i,j)|x_j| = R_n(i) \max_{1\leq j \leq n} |x_j|.
    \end{align*}
    \end{enumerate}
    
    For the third part, we first use the Rayleigh quotient. For any nonzero vector $v\in\mathbb R^n$,
    \begin{align*}
      \lambda_n \;=\;\max_{\|u\|_2=1}u^T A_n\,u
      \;\ge\;
      \max_{\|u\|_2=1} \sum_{i,j=1}^n A_n(i,j)u_i u_j\\
      \;\geq\;\sum_{i,j=1}^n A_n(i,j)\frac{1}{\sqrt{n}}\frac{1}{\sqrt{n}}
      \;=\;
      \frac1n\sum_{i,j}A_n(i,j) \to 1.
    \end{align*}
    For the other direction, we use part two. Choose $k$ such that $|x_k| = \max_{j} |x_j|$
    \begin{align*}
        \lambda_n \leq &\frac{x_k}{x_k} R_n(k) \to 1
    \end{align*}
\end{solution}
\begin{bbox}{(Straightforward) {2021 May Exam, \#7}}
    Suppose that $A=(a_{ij})_{1\le i,j \le 2}$ is a $2\times 2$ symmetric matrix, with $a_{11} = a_{22} =\frac{3}{4}$ and $a_{12} = a_{21} = \frac{1}{4}$. 
    \begin{itemize}
        \item Find the eigenvalues and eigenvectors of the matrix $A$.
        \item Compute $\lim_{n\to +\infty} a_{12}^{(n)}$ where $a_{i,j}^{(n)}$ denotes the $ij$th entry of the matrix $A^n$.
    \end{itemize}
\end{bbox}
\begin{solution}
    The first part is standard. Set up the characteristic polynomial and solve for its roots:
    \[
    p(\lambda) = \det(A-\lambda I) = 0 \implies \lambda = \frac{1}{2}, 1
    \]
    The eigenvector corresponding to $\lambda=1$ is $\begin{bmatrix}
         1/\sqrt{2}  \\
         1/\sqrt{2} 
    \end{bmatrix}$. The eigenvector corresponding to $\lambda=\frac{1}{2}$ is $\begin{bmatrix}
        1/\sqrt{2}\\ -1/\sqrt{2}
    \end{bmatrix}$.
    \newline
    For the second part. We should use diagonalization; otherwise, matrix exponential would be hard to compute.
    \[
    A = PDP^{-1}
    \]
    where $D$ is the diagonal matrix whose diagonal entries are the eigenvalues. $P^{-1}$ is the matrix whose the columns are the corresponding eigenvectors. So $D = \begin{bmatrix}
        1 & 0\\
        0 & \frac{1}{2}
    \end{bmatrix}$ and $ P^{-1} = \begin{bmatrix}
        1 & 1\\
        1 & -1
    \end{bmatrix}, \quad P= \begin{bmatrix}
        \frac{1}{2} & \frac{1}{2}\\
        \frac{1}{2} & -\frac{1}{2}
    \end{bmatrix}$. 
    \begin{align*}
    A^n &= P \begin{bmatrix}
        1^n & 0\\
        0 & \frac{1}{2}^n
    \end{bmatrix} P^{-1} = \begin{bmatrix}
        \frac{1}{2} & \frac{1}{2}\\
        \frac{1}{2} & -\frac{1}{2}
    \end{bmatrix} \begin{bmatrix}
        1 & 0\\
        0 & 0.5^n
    \end{bmatrix} \begin{bmatrix}
        1 & 1\\
        1 & -1
    \end{bmatrix}\\
    &= \begin{bmatrix}
        \frac{1}{2} & \frac{1}{2}\\
        \frac{1}{2} & -\frac{1}{2}
    \end{bmatrix} \begin{bmatrix}
        1 & 1\\
        0.5^n & -(0.5^n)
    \end{bmatrix}\\
    a_{12}^n = \frac{1}{2} - \frac{1}{2} \cdot (-(0.5^n)) \to \frac{1}{2}.
    \end{align*}
    This question is straightforward in my opinion!
\end{solution}
\begin{bbox}{{2021 Sept Exam, \#6}}
Let $A \in \mathbb R^{m\times n}$ be an $m\times n$ matrix with $n < m$. Suppose that $\lambda_1,\lambda_2,\dots, \lambda_n$ and $\vec v_1,\dots, \vec v_n$ are the eigenvalues and eigenvectors of $A^TA$. What can we say about ALL the eigenvalues and eigenvectors of $AA^T$. Justify your answer.
\end{bbox}
\begin{solution}
    When it comes $AA^T$, especially when $A$ is non-symmetric or even non-square, we should think of Singular Value Decomposition SVD! Let $A = U \Sigma V^{-1}$ be its SVD. Then $A^T = V \Sigma^{T} U^{-1}$.
    The singular values of $A$ are the square root of the eigenvalues of $AA^T$, and we see that $A$ and $A^T$ share the same singular values. Note that $U$ is composed of orthonormal eigenvectors of $AA^T$ and $V$ is composed of orthonormal eigenvectors of $A^TA$. $AA^T \vec v_i = \lambda \vec v_i $
\end{solution}
\begin{bbox}{Eigenvalue of Orthogonal Matrix}
    Let $A$ be a $3\times 3$ real-valued matrix such that $A^T A = AA^T = I_3$ and $\det (A) = 1$. Prove that $1$ is an eigenvalue of $A$.
\end{bbox}
\begin{solution}
    % Well, our first intuition should be that $1$ must a root of the characteristic polynomial. The problem wants to tell us that $A$ is orthogonal. The problem also told us something about the determinant, which kinda means that we should consider the characteristic polynomial. Okay, consider 
    % \[
    % \det (A - I) = 0 \iff \det(A^T(A-I))=0 \iff \det(I - A^T) =0
    % \]
    Since the problem wants to tell us that $A$ is orthogonal, we should be thinking of the length-preserving property. Let $\lambda$ be an eigenvalue of $A$ and $\vec v$ be a corresponding unit eigenvector. Then 
    \[
    \|A\vec v\| = \sqrt{\vec v^T A^T A \vec v} = 1 =\|\lambda \vec v\| = |\lambda|
    \]. 
    \newline
    The determinant is the product of the eigenvalues and $-1$ cannot be the only eigenvalue of $A$ because $(-1)^3 = -1 \neq 1 = \det A$.
\end{solution}
\begin{bbox}{(Straightforward) Trace of the square of a symmetric matrix is zero means zero matrix}
    Let $A$ be an $n\times n$ symmetric matrix such that $\Tr(A^2) = 0$. Show that $A = 0_{n\times n}$.
    \newline
    Hint: Use the fact that $\Tr(ABC) = \Tr(CAB)$.
\end{bbox}
\begin{solution}
    The hint apparently wants us to apply the spectral theorem to obtain a diagonalization $A = Q \Lambda Q^T$.
    \[
    \Tr A^2 = \Tr \left(Q\Lambda^2 Q^T\right) = \Tr\left(Q^T Q\Lambda^2\right) =\Tr(\Lambda^2)= 0.
    \]
    The trace is equal to the sum of the eigenvalues (to be honest, with this fact, we don't really need the hint), i.e. the diagonal of $\Lambda^2$ is zero. Since the entries of $\Lambda^2$ are non-zero, $\Lambda^2 = 0$ and hence $\Lambda = 0$. Therefore $A=0$.
\end{solution}

\begin{bbox}{Eigenvectors are the same iff Multiplication commutes}
    Let $A,B\in \mathbb R^{n\times n}$ have respective eigendecompositions $Q_1 D_1Q_1^T$ and $Q_2 D_2 Q_2^T$ (recall this means each $D_i$ is a diagonal matrix of eigenvalues and each $Q_i$ is an orthogonal matrix). Prove that $Q_1 = Q_2$ if and only if $AB = BA$. You may assume that $A, B$ do not have any repeated eigenvalues.
\end{bbox}
\begin{solution}
    Suppose $AB = BA$, consider an eigenpair $\lambda$ and $\vec v$ of $A$. 
    \[
    BA \vec v = \lambda B\vec v. = AB\vec v.
    \]
    This means that $B\vec v$ is an eigenvector of $A$ corresponding to the eigenvalue $\lambda$. This then imply to $A$ and $B$ share the same set of eigenvalues $\lambda_i$ with corresponding eigenvectors $\vec v_i$ and $B\vec v_i$. For $Q_1 = Q_2$, we need to show that $\vec v_i \propto B\vec v$: 
    \[
    A B\vec v = \lambda B\vec v, \implies B\vec v \propto \vec v
    \] since the eigenspaces of $A$ are all one-dimensional.
    \newline
    The other direction is easier. Suppose $Q_1 = Q_2$, then 
    \[
    AB = Q_1 D_1 Q_1^T Q_2 D_2 Q_2^T = Q_2 D_2 Q_2^TQ_1 D_1 Q_1^T = BA
    \]
\end{solution}
\begin{bbox}{(Straightforward) Eigenvalue of $uv^T$}
    Let $A = uv^T \in \mathbb R^{n\times n}$ be a rank-one matrix, i.e. $u, v \in \mathbb R^n$. Suppose $u,v \neq 0_n$. Find, with proof, all the eigenvalues of $A$.
\end{bbox}
\begin{solution}
    Let $\lambda$ be an eigenvalue of $A$ and $\vec x$ be a corresponding eigenvector, then 
    \[
    A \vec x= uv^T \vec x = \lambda \vec x 
    \]
    Note that 
    \[ 
    u v^T x = u \langle v, x\rangle = \lambda \vec x
    \]
    This means that $\vec x$ and $\vec u$ share the same direction. So 
    \[
    A \vec u = u v^T u = \lambda u
    \]
    Therefore, 
    \[
    \lambda = \vec v^T \vec u
    \]
    There can be no other eigenvalues because $A$ has rank-one.
    \newline
    Comment: Should find this problem straightforward.
\end{solution}
\begin{bbox}{Heisenberg Uncertainty Principle}
    Suppose $A, B \in \mathbb R^{n\times n}$ are symmetric matrices satisfying $AB + BA = Id$. Show that for all vectors $v\in \mathbb R^n\backslash\{0_n\}$,
    \[
    \max \{\frac{\|Av\|_2}{\|v\|_2}, \frac{\|Bv\|_2}{\|v\|_2}\} \geq 1/\sqrt{2}.
    \]
\end{bbox}
\begin{solution}
    This seems to be an interesting problem. Here's my thought process (which turned out to be misleading tho, because the canonical solution is simple): since the problem mentioned $A,B$ are symmetric and we are apparently dealing with Rayleigh quotient, and a maximization problem, so we should definitely look at the eigenvalue of $A,B$.
    \begin{enumerate}
        \item Since \(A\) and \(B\) are real symmetric, so is
            \[
            (A - B)^2.
            \]
            But for any symmetric \(X\),  
            \[
            X^2\;\succeq\;0
            \qquad\bigl(\text{i.e.\ }v^T X^2 v = \|Xv\|^2\ge0\bigr).
            \]
            Hence
            \[
            (A-B)^2 \;\succeq\;0
            \;\Longrightarrow\;
            A^2 - (AB+BA) + B^2 \;\succeq\;0
            \;\Longrightarrow\;
            A^2 + B^2 \;\succeq\;(AB+BA)\;=\;I.
            \]
            \item Loewner order \(A^2+B^2\succeq I\) means all eigenvalues of \(A^2+B^2\) are \(\ge 1\).  Equivalently, for every \(v\)
            \[
            v^T(A^2+B^2)\,v
            \;\ge\;v^T v
            \;\Longrightarrow\;
            \|Av\|^2 + \|Bv\|^2 \;\ge\;\|v\|^2.
            \]
            \item Finally,
                \[
                \max\{\|Av\|^2,\;\|Bv\|^2\}
                \;\ge\;\frac{\|Av\|^2 + \|Bv\|^2}{2}
                \;\ge\;\frac{\|v\|^2}{2},
                \]
                so
                \[
                \max\Bigl\{\frac{\|Av\|}{\|v\|},\,\frac{\|Bv\|}{\|v\|}\Bigr\}
                \;\ge\;\frac1{\sqrt 2},
                \]
    \end{enumerate}

    The canonical clean solution is actually to consider the inner product
    \begin{align*}
        \|v\|_2^2 =v^T v &= v^T I v = v^T(AB+BA)v\\
        &= v^T AB v + v^T BA v\\
        &\leq 2 |\langle Av, Bv \rangle| \\
        &\le 2 \|Av\|_2 \|Bv\|_2    
        \end{align*}
        where the last inequality is by Cauchy schwarz.
        \newline
        Finally, one of $\frac{\|Av\|_2}{\|v\|_2}$ and $\frac{\|Bv\|_2}{\|v\|_2}$ must be greater than $\frac{1}{\sqrt{2}}$. 
        \newline
        Comment: This teaches us a lesson that whenever we see the $2$-norm, consider playing with $v^Tv$. I was too obsessed with eigenvalues...
\end{solution} 
\begin{bbox}{Invertibility }
    Let $A=(a_{ij})$ be an $n\times n$ real matrix whose diagonal entries $a_{ii}$ satisfy $a_{ii}\ge 1$ for all $i$. Suppose also $\sum_{i\ne j} a_{ij} < 1$. Prove that the inverse matrix $A^{-1}$ exists.
\end{bbox}
\begin{solution}
    I gave myself the hint that I can use proof by contradiction since the statement is boolean.
    \begin{enumerate}
        \item Suppose by contradiction that $A$ is not invertible. Then the kernel of $A$ is non-trivial. (I did this step correctly). 
        \item We know that the diagonal entries are all greater than $1$ and the sum of all non-diagonal entries is less than $1$.
        \item This means that there exists $\vec v \ne \vec 0$ such that $A\vec v = \vec 0$ and assume that $\|\vec v\| = 1$. Then
        \begin{align*}
            &\sum_{j=1}^n a_{ij} v_j  =0\\
            &\implies a_{ii}v_{i} = -\sum_{j:j\ne i}a_{ij} v_j\\
            &\implies \sum_{i=1}^n a_{ii}^2 v_i^2 = \sum_{i=1}^n (\sum_{j:j\neq i}a_{ij}v_j)^2\\
        \end{align*}
        \item However, \[
        \sum_{i=1}^n a_{ii}^2 v_i^2\ge 1
        \]
        but 
        \begin{align*}
        &\sum_{i=1}^n(\sum_{j:j\ne i}a_{ij}v_j)^2 \le \sum_{i=1}^n \sum_{j:j\ne i}(a_{ij}^2) \sum_{j:j\ne i}(v_j)^2 = \sum_{i=1}^n \sum_{j: j\ne i} a_{ij}^2 < 1
        \end{align*}
    \end{enumerate}
\end{solution}
\begin{bbox}{Greshgorin Circle Problem}
    Let $A\in \mathbb C^{n\times n}$ with entries $a_{ij}$. Let \[
    R_i := \sum_{j\neq i} |a_{ij}|
    \]
    Let $D(a_{ii}, R_i)$ be a closed disc centered at $a_{ii}$ with radius $R_{i}$, called the Greshgorin disc. Show that every eigenvalue of $A$ lies at least in one of the Goshgorin discs of $A$.
    \end{bbox}
    \begin{solution}
        This appears to be a hard and interesting problem. My first intuition is proof by contradiction.
        \begin{enumerate}
            \item Suppose there exists $\lambda$ (with eigenvector $\vec v$) that doesn't lie in any Goshgorin disc: for all $i$
            \[
            |\lambda - a_{ii}| > R_i = \sum_{j\ne i} |a_{ij}|
            \]
            \item We need to reach a contradiction out of this. Let me try considering eigen vectors $\vec v$ such that the absolute value of the entries is upper bounded by $1$. Then 
            \begin{align*}
                &A v = \lambda v \\
                &\sum_{j=1}^n a_{ij}v_j=\lambda v_i\\
                &\sum_{j:j\ne i}a_{ij}v_j + a_{ii}v_i  = \lambda v_i\\
            \end{align*}
            Then we get 
            \[
            \sum_{j:j\neq i}|a_{ij}| \ge |\sum_{j:j\ne i}a_{ij}| = |v_i(\lambda-a_{ii})| = |\lambda-a_{ii}|
            \]
            which is a contradiction.
        \end{enumerate}
    \end{solution}
    
\begin{bbox}{Limit of $A^n$}
    Let $A = \begin{bmatrix}
        1 & 0 & 0\\
        1/2 & 1/2 & 0\\
        1/3 & 1/3 & 1/3
    \end{bmatrix}$. Find $\lim_{n\to \infty} A^n$. Hint: the eigenvalues of a lower triangular matrix are its diagonal entries.
\end{bbox}
\begin{solution}
    This is a typical diagonalization problem. From the hint, we know that the eigenvalues are $1$, $1/2$, and $1/3$. 
    \begin{itemize}
        \item Solving $\begin{bmatrix}
            1 & 0 & 0\\
        1/2 & 1/2 & 0\\
        1/3 & 1/3 & 1/3
        \end{bmatrix} \begin{bmatrix}
             x_1  \\ x_2 \\ x_3
        \end{bmatrix} = \begin{bmatrix}
             x_1  \\ x_2 \\ x_3
        \end{bmatrix}$, We get that $\frac{1}{2}x_1 + \frac{1}{2}x_2 =x_2\implies x_1 = x_2$. From the last equation, we know that $x_1=x_2=x_3$. An eigenvector is just $\begin{bmatrix}
            1\\1\\1
        \end{bmatrix}$
        \item Solving $\begin{bmatrix}
            1 & 0 & 0\\
        1/2 & 1/2 & 0\\
        1/3 & 1/3 & 1/3
        \end{bmatrix} \begin{bmatrix}
             x_1  \\ x_2 \\ x_3
        \end{bmatrix} = \frac{1}{2}\begin{bmatrix}
             x_1  \\ x_2 \\ x_3
        \end{bmatrix}$
    \end{itemize}
\end{solution}
\begin{bbox}{$AB$ and $BA$ have the same eigenvalues}
    Show that $AB$ and $BA$ have the same eigenvalues if $A$ is invertible.
\end{bbox}
\begin{solution}
    The proof is quite short. Let $\lambda$ be an eigenvalue of $AB$ and $\vec v_i$ be corresponding eigenvectors. Then 
    \begin{align*}
        AB \vec v &= \lambda \vec v\\
        \implies B\vec v &= \lambda A^{-1}\vec v\\
        \implies B A (A^{-1}\vec v) &= B\vec v = \lambda (A^{-1}\vec v)
    \end{align*}
    This shows that $\lambda$ is an eigenvalue of $BA$ with the same multiplicity since $A^{-1}$ is also an invertible linear transformation.
    \newline
    An alternative solution shows that $AB$ and $BA$ share the same characteristic polynomial:
    \[
    \det(AB - \lambda I) = \det(A^{-1}A(AB-\lambda I)) = \det(A^{-1}(AB-\lambda I)A) = \det(BA-\lambda I)
    \]
\end{solution}
\begin{bbox}{Invertibility Exercise}
    Let $A = (a_{ij})$ be a $2\times 2$ real matrix such that 
    \[
    a_{11}^2 + a_{12}^2 + a_{21}^2 + a_{22}^2 < \frac{1}{1000}
    \]
    Show that $I + A$ is invertible.
\end{bbox}
\begin{solution}
    My intuition is to look at the determinant of $I + A$ since the determinant of a two by two matrix is very easy to compute.
    \begin{align*}
    \det(I+A) &= (1+a_{11})(1+a_{22})-a_{12}a_{21}\\
    &=1 + a_{11}+ a_{22}+a_{11}a_{22}-a_{12}a_{21}
    \end{align*}
    One thing we can do is to use the fact that $|a_{ij}|\leq \frac{1}{\sqrt{1000}}\le \frac{1}{10}$. The statement in the problem is true even if \[
     a_{11}^2 + a_{12}^2 + a_{21}^2 + a_{22}^2 < 1
    \]
    \newline
    An alternative solution: Suppose by contradiction that $I+A$ has a non-trivial kernel, i.e. there exists $\vec v$ such that $(I+A)\vec v = 0$. Then 
    \[
    \begin{cases}
        v_1 + a_{11} v_1 + a_{12}v_2 = 0\\
        a_{21}v_1 + v_2 + a_{22}v_2 = 0
    \end{cases}
    \]
    The next key step is to look at the norm of $\vec v$, which is 
    \[
    \|\vec v\|^2_2 = (a_{11}v_1 + a_{12}v_2)^2 +(a_{21}v_1 + a_{22}v_2)^2 \leq (a_{11}^2 + a_{12}^2 + a_{21}^2 + a_{22}^2)(v_{11}^2 + v_{22}^2) < \frac{1}{1000}\|\vec v\|_2^2
    \]
    The first inequality is true because 
    \[
    (ax+by)^2 \leq (a^2+b^2)(x^2+y^2)
    \]
    Note that this is actually Cauchy schwarz.
\end{solution}
\begin{bbox}{Eigenvalue and Optimization Exercise}
    Let $A\in \mathbb R^{n\times n}$ real symmetric matrix and let $\lambda_1\ge \dots \ge \lambda_n$ be its eigenvalues in descending order. Show that 
    \[
    \lambda_k \le \max_{U: \dim(U) = k}\min_{x\in U: \|x\|_2 = 1} \langle Ax, x\rangle
    \]
    The maximum above is over all $k$-dimensional subspaces $U$ of $\mathbb R^n$. Hint: form an orthonormal basis of eigenvectors to make $U$.
\end{bbox}
\begin{solution}
    Following the solution, let $\{\vec v_i\}$ be an orthonormal basis of eigenvectors of $U$, corresponding to the eigenvalues $\lambda_i$. The existence of such basis is by the Spectral Theorem.
    Let $U$ be the direct sum of the first $K$ eigenspaces. For any $x\in U$, $x = \sum_{i=1}^k a_i v_i$.
    Let $\|x\|_2 = 1$. Then 
    \[
    \langle Ax, x\rangle = \langle \sum^k_{i=1} a_i \lambda_i v_i, \sum_{i=1}^k a_i v_i\rangle = \sum_{i=1}^k \lambda_i a_i^2 \ge \sum_{i=1}^k a_i^2\lambda_k = \lambda_k
    \]
\end{solution}
\begin{bbox}{Projection Exercise}
    For a vector $\vec v \in \mathbb R^n \backslash \{0\}$, define the map $F: \mathbb R^n \to \mathbb R^n$ by 
    \[
    F(x) = \argmin_{z\in \text{Span}(\vec v)}\|z-x\|_2. 
    \]
    Compute $F$ explicitly in terms of $\vec v$. Is $F: \mathbb R^n \to \mathbb R^n$ a linear transformation?
\end{bbox}
\begin{solution}
    \begin{align*}
        F(x) &= \argmin_{a}\|a\vec v - \vec x\|_2
    \end{align*}
    We want to minimize the distance between $a\vec v$ and $\vec x$. We should just project $x$ onto the Span of $\vec v$ and $F$ should return
    \[
    F(x) = \frac{v^Tx}{\|v\|_2\|x\|_2} \|x\|_2\frac{\vec v}{\|v\|_2} = \frac{v^T x}{\|v\|_2^2}v
    \]
    This is apparently a linear transformation because projection is linear.
    \newline
    Some words on simplification:
    \[
    v^T x v = v(v^T x)=(vv^T)x
    \] since scalar-vector multiplication is commutative.
\end{solution}
\begin{bbox}{(Straightforward)Square Root of a Symmetric Matrix}
    Suppose $A \in \mathbb R^{n\times n}$ and $A=A^T$ with all eigenvalues of $A$ being positive. Show there exists a matrix $B$ such that $B^2 = A$.
\end{bbox}
\begin{solution}
    $A=A^T$ means that $A$ is symmetric, then by the spectral theorem, $A$ can be diagonalized into 
    \[
    Q \Lambda Q^T
    \]
    where the digonal of $\Lambda$ consists of the eigenvalues of $A$, which are all posiitive and thus we can take square roots of them. Therefore, let $\sqrt{\Lambda}$ denote the diagonal matrix with the square root of the eigenvalues of $A$ on the diagonal. Since the square of a diagonal matrix can be obtained by squaring the diagonal entries, we see that $\sqrt{A}^2 = A$. We can define the matrix $B$ as $Q\sqrt{\Lambda}Q^T$ and 
    \[
    B^2 = Q\sqrt{\Lambda}Q^TQ\sqrt{\Lambda}Q^T = Q\Lambda Q^T = A
    \]
\end{solution}
\begin{bbox}{$|\det|$ is equal to product of singular values}
    Let $A\in \mathbb R^{n\times n}$ and let $\{\sigma_i\}_{i=1}^n$ be the singular values of $A$. Show that $|\det(A)| = \prod_{i=1}^n \sigma_i$ 
\end{bbox}
\begin{solution}
    Don't work with characteristic polynomial for this one. We should definitely consider the SVD of $A$ which is of the form
    \[
    A = U\Sigma V^T
    \]
    where $U$ and $V^T$ are orthonormal matrices. Then
    \[
    \det A = \det(U\Sigma V^T) = \det U \det \Sigma \det V^T = \pm \det (\Sigma)
    \]
    Note that the determinant of orthonormal matrices is either $+1$ or $-1$, since $\det (Q)\det(Q^T) = \det(I)$ and transposition does not change the determinant.
\end{solution}
\begin{bbox}{Low-Rank Matrix Approximation}
    Let $A\in \mathbb R^{m \times n}$ and for a positive integer $p < rank(A)$, define $A_p = \sum_{i=1}^p \sigma_i u_i v_i^T$ where $\sigma_i$ is the $i$th largest singular value of $A$, and $u_i, v_i$ are respective left and right singular vectors, i.e. the SVD is $A = U\Sigma V^T$. Then prove
    \[
    \sup_{x:\|x\|_2 = 1} \|(A-A_p)x\|_2 = \sigma_{p+1}
    \]
\end{bbox}
\begin{solution}
    \begin{align*}
        A-A_p &= \sum_{i=1}^n \sigma_i u_i v_i^T - \sum_{i=1}^p \sigma_i u_iv_i^T\\
        &= \sum_{i=p+1}^n \sigma_iu_iv_i^T\\
        \|(A-A_p)\vec x\|_2^2 &= \left(\sum_{i=p+1}^n \sigma_iu_iv_i^T \vec x \right)^T\left(\sum_{j=p+1}^n \sigma_ju_jv_j^T \vec x\right)\\
        &=\left(\sum_{i=p+1}^n \sigma_i x^Tv_i u_i^T \right) \left(\sum_{j=p+1}^n \sigma_ju_jv_j^Tx\right)\\
        &= \sum_{i=p+1}^n\sigma_i^2 x^T v_i u_i^T u_i v_i^T \vec x \quad \text{by orthonormality of $\vec u, \vec v$}\\
        &= \sum_{i=p+1}^n \sigma_i^2 x^T v_i v_i^T x\\
        &= \sum_{i=p+1}^n \sigma_i^2 (v_i^Tx)^2
    \end{align*} 
    Note that this is a convex combination of $\sigma_i^2$. To maximize it, we can let $\|v_{p+1}^Tx_{p+1}\|=1$
    Alternatively, we can argue that $U^T(A-A_p) V = diag(0,\dots, 0,\sigma_{p+1},\dots)$. By orthgonal invariance of the $2$-norm, we have 
    \[
    \|(A-A_p)x\|_2 = \|U^T(A-A_p)Vx\|_2
    \]
    which is equal to the biggest singular value of $U^T(A-A_p)V$, i.e. $\sigma_{p+1}$. 
\end{solution}

\begin{bbox}{Eigenvalues of Symmetric Idempotent Matrix}
    Suppose $P\in \mathbb R^{n\times n}$ symmetric matrix that satisfies $P^2 = P$, a so-called idempotent matrix. Find all the eigenvalues of $P$ with their algebraic multiplicities in terms of $P$.
\end{bbox}
\begin{solution}
    Since $P$ is symmetric, we can decompose it into $Q \Lambda Q^T$ by the spectral theorem. Then we know that 
    \[
    P^2 = Q\Lambda Q^T Q \Lambda Q^T = \Lambda^2 = P
    \]
    Note that the eigenvalues of $P$ are either $1$ or $0$.
    \newline
    We can also consider an eigenpair $\lambda, \vec v$ of $P$, then 
    \[
    P\vec v = P^2 \vec v = \lambda\vec v = \lambda^2 \vec v.
    \]
    The eigenvalue $1$ of $P$ has multiplicity of $rank (\Lambda) = rank(P)$ since multiplication by an invertible matrix does not change the rank. Also, we then have that the multiplicity of the eigenvlue $\lambda=0$ is $n-rank(P)$.
\end{solution}
\begin{bbox}{Singular Covariance Matrix means Linearly Dependent a.s.}
    Suppose that $\Sigma$ is the covariance matrix of $k$ zero-mean random variables $X_1,\dots, X_k$, i.e. if $X=(X_1, \dots, X_k)$, then $\Sigma:= \mathbb E[XX^T]$. Prove that if $\Sigma$ is singular, then $X_1,\dots, X_k$ are linearly dependent almost everywhere.
\end{bbox}
\begin{solution}
    $\Sigma$ is a symemtric positive definite matrix. If $\Sigma$ is singular, then it has zero eigenvalue. Let's $\vec v$ be the corresponding eigenvector. Since $\Sigma \vec v=\vec 0$, $v^T \Sigma v = 0$. Then
    \begin{align*}
    &v^T \Sigma v = 0\implies v^T \mathbb E[XX^T]v=0\\
    &\mathbb E[v^TXX^Tv] = 0\\
    &= \mathbb E ((X^T v)^2) = 0
    \end{align*}
    $(X^Tq)^2$ is a non-negative random variable with mean zero, it must be zero almost surely for its expectation to be zero.
    \newline
    Another arguemnt we can use is that a random variable has variance zero iff it's almost surely constant.
\end{solution}
\end{document}