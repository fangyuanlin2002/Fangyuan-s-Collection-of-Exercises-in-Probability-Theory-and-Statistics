\documentclass[../main]{subfiles}
\begin{document}
\chapter{Linear Algebra}
\section{Eigenvalue and Eigenvector}
\begin{bbox}{The eigenvalues of $A^2$}
    If $A$ has eigenvalues $\lambda_i$. Then the eigenvalues of $A^2$ are $\lambda_i^2$.
    \begin{proof}
        Well, my first intuition is to think about the diagonalization of $A$ and the result becomes clear.
        \newline
        A rigorous proof is also not hard: 
        \begin{enumerate}
            \item \begin{align*}
                A\vec v = \lambda \vec v &\implies A^2 \vec v = A \lambda\vec v = \lambda^2 \vec v.
            \end{align*}
        \item 
            The algebraic multiplicity of the eigenvalues $\lambda^2_i$ of $A^2$ is the same as the eigenvalues $\lambda_i$ of $A$:
            \[
            \det (A^2 - \lambda^2 I) = \det (A+\lambda I)\det (A-\lambda I).
            \]
            This means that
        \end{enumerate}
    \end{proof}
\end{bbox}

\section{Orthogonality}
Orthogonal Matrices have the following properties:
\begin{itemize}
    \item $U^{-1}=U^T$
    \item Rows and columns are orthogonal unit vectors.
    \item Preserves the inner product of vectors: $\langle x,y\rangle = \langle Ux, Uy\rangle$.
    \item Isometric: length/distance preserving.
    \item Rigit rotation, reflection, rotoreflection.
\end{itemize}

\section{Singular Value Decomposition}
The Principal Component Analysis PCA is a byproduct of SVD. Let $X$ denote the original data matrix. Let $X^*$ be the (column-)centered matrix. (In machine learning, each row represents a data point and each column represents a feature). Let $\hat X$ denote the centered, normalized matrix.
\begin{itemize}
    \item $X^{*T}X^*$ is the covariance matrix.
    \item $\hat X^T \hat X$ is the correlation matrix.
    \item $X^T X$ is the cross-product.
\end{itemize}
Consider an $n\times d$ matrix $A$
\begin{bbox}{Singular Value Decomposition}
    Let $A$ be an $n\times d$ matrix with singular vectors $v_1,\dots, v_r$ and corresponding singular values $\sigma_1,\dots, \sigma_r$. Then $u_i = \frac{1}{\sigma_i} Av_i$ are the left singular vectors and $A$ can be decomposed into a sum of rank one matrices as 
    \[
    A = \sum_{i=1}^r \sigma_i u_i v_i^T
    \]
    \begin{itemize}
        \item Compute $A^T A$. This is a symmetric and positive semi-definite matrix, so it has orthonormal eigenvectors $v_i$ and non-negative eigenvalues $\lambda_i$ by the Spectral theorem.
        \item The singular values are $\sigma_i = \sqrt{\lambda_i}$
        \item $V = [v_i]$ is the matrix of right singular values.
        \item Define $u_i = \frac{1}{\sigma_i}Av_i$
    \end{itemize}
\end{bbox}
\begin{pbox}{Intuition behind SVD}
    Consider the optimization problem:
    \[
    \max_{\|v\|=1} \|Av\| = \max_{\|v\|=1} v^T A^T Av
    \]
    This is a Rayleigh quotient of $A^T A$, so its maximum value corresponds to its largest eigenvalue!
    \begin{itemize}
        \item If we define $u_i = \frac{1}{\sigma_i}Av_i$, then we get $Av_i = \sigma_i u_i$ and $u_i$ are orthonomal vectors.
    \end{itemize}
    Let's verify the algorithm is correct:
    \[
    u_i^T A v_i = (\frac{1}{\sigma_i} Av_i)^TAv_j = \frac{\sigma_i^2}{\sigma_i} = \sigma_i
    \]
\end{pbox}
\begin{itemize}
    \item We can think of $A$ as three steps: rotation $V^T$, then horizontal/vertical scaling ($\Sigma$), lastly rotation $U$.
    \item Use \begin{verbatim}
        from scipy import linalg
        U, s, Vh = linalg.svd(X)
    \end{verbatim}
    to compute the SVD of matrix $X$.
    \item Any matrix can be quickly decomposed into SVD form.
    \item One important and obvious application of SVD is data compression.
\end{itemize}
\begin{pbox}{Apply SVD to a two by two matrix}
    Let $M = \begin{bmatrix}
        0 & 1\\
        -1 & 0
    \end{bmatrix}$, rotation by ninety degrees clockwise. Note that this matrix has complex eigenvalue of $-i$. Let's compute the SVD of it.
    \[
    SVD(M) = \begin{bmatrix}
        0 & 1\\
        1 & 0
    \end{bmatrix} \begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix}
    \begin{bmatrix}
        -1 & 0 \\
        0 & 1
    \end{bmatrix}
    \]
    
    \begin{itemize}
        \item $V^T$ is reflection about the $y$-axis / rotation by 180 degress about $y$-axis.
        \item $\Sigma$ is the trivial scaling.
        \item $U$ is rotation by 180 degress about the origin. (reflection about both the $x$ and $y$-axes).
        \item The rank $1$ approximation would be $\begin{bmatrix}
            0 & 0\\
            -1 & 0
        \end{bmatrix}$
    \end{itemize}
\end{pbox}
\section{Determinant}
\subsection{The Determinant is the Product of Eigenvalues}
\begin{proof}
    Let $A$ be a matrix with eigenvalues $\lambda_i$. The key idea of the proof uses the characteristic polynomial.
    \begin{enumerate}
        \item Consider the characteristic polynomial
        \[
        p(\lambda) = |\lambda I - A| = c_0 + c_1 \lambda + \dots + \lambda^n
        \]
        Note that the characteristic polynomial is monic.
        \item We can obtain $c_0$ by
        \[
        p(0) = c_0 = |0\cdot I - A| = (-1)^n \det A
        \]
        \item Note that the eigenvalues $\lambda_i$ are roots of the characteristic polynomial so 
        \[
        p(0) = \prod_i(0-\lambda_i) = (-1)^n \lambda_i
        \]
        \item Lastly, 
        \[
        c_0 = (-1)^n \prod_{i}\lambda_i  = (-1)^n \det A
        \]
        so 
        \[
        \det A = \prod_i \lambda_i
        \]
    \end{enumerate}
\end{proof}
\section{Trace}
\subsection{Trace is Equal to the Sum of Eigenvalues}
\begin{bbox}{Trace is Equal to the Sum of Eigenvalues}
    Let $A$ be an $n\times n$ matrix with eigenvalues $\lambda_i$. Show that 
    \[
    \Tr(A) = \sum_i \lambda_i
    \]
\end{bbox}
\begin{solution}
    \begin{enumerate}
        \item The proof is similar to that of "the determinant is product of eigenvalues," i.e. we work with the characteristic polynomial. \textcolor{red}{TO BE FILLED IN} 
    \end{enumerate}
\end{solution}
\subsection{An Inequality relating Trace and Determinant}
\begin{bbox}{{2018 Summer Practice Problem, \# 18}}
Suppose $\Sigma$ is a non-negative definite matrix of $n\times n$ real entries and real eigenvalues. Show that 
\[
\Tr(\Sigma^2) \geq n \cdot \det(\Sigma)^{2/n}.
\]
\end{bbox}
\begin{solution}
    \begin{enumerate}
        \item Let $\{\lambda_i\}$ be the eigenvalues of $\Sigma$. To make some progress, let's write the trace as 
        \[
        \Tr(\Sigma) = \sum_i \lambda_i^2        
        \]
        \item By the Arithmetic Mean - Geometric Mean inequality,
        \[
        \frac{\sum_{i=1}^n \lambda_i^2}{n} \geq \sqrt[n]{\prod_{i=1}^n \lambda_i^2} \implies \Tr(\Sigma^2) \geq n \det(\Sigma)^{\frac{2}{n}}
        \]
    \end{enumerate}
\end{solution}
\section{Core Competency Exam Questions}
\begin{bbox}{{2020 September Exam, \#8}}
    For every $n\geq 1$, let $A_n$ be an $n\times n$ symmetric matrix with non-negative entries. Let $R_n(i):= \sum_{j=1}^n A_n(i,j)$ denote the ith row/column sum of $A_n$. Assume that 
    \[
    \lim_{n\to \infty} \max_{1\leq i \leq n} |R_n(i) - 1| = 0.
    \]
    Let $\lambda_n \geq 0$ denote an eigenvalue with the largest absolute value, and let $\vec x = (x_1, \dots, x_n)$ denote its corresponding eigenvector. 
    \begin{itemize}
        \item Show that 
        \[
        \frac{1}{n} \sum_{i,j=1}^n A_n(i,j) \to 1
        \]
        \item Show that $\lambda_n |x_i| \leq \max_{1\leq j \leq n} |x_j| R_n(i)$.
        \item Using parts one and two, show that 
        \[
        \lambda_n \to 1.
        \]
    \end{itemize}
\end{bbox}
\begin{solution}
    For the first part, let's just write something down:
    \begin{enumerate}
        \item 
        \begin{align*}
        \frac{1}{n} \sum_{i,j=1}^n A_n(i,j) &= \frac{1}{n} \sum_{i=1}^n R_n(i)
    \end{align*}
        \item \begin{align*}
        \left|\frac{1}{n} \sum_{i,j=1}^n A_n(i,j) - 1\right| &= \left|\frac{1}{n} \sum_{i=1}^n R_n(i) - 1\right|\\
        &\leq \max_{1\leq i\leq n} |R_n(i) - 1| \to 0
    \end{align*}
    \end{enumerate}
    
    For the second part, 
    \begin{enumerate}
        \item By assumption, \begin{align*}
        A_n \vec x &= \lambda_n \vec x, \quad \lambda_n x_i = \sum_{j=1}^n A_n(i,j)x_j\\
        \lambda_n |x_i|&\leq \sum_{j=1}^n A_n(i,j)|x_j| = R_n(i) \max_{1\leq j \leq n} |x_j|.
    \end{align*}
    \end{enumerate}
    
    For the third part, we first use the Rayleigh quotient. For any nonzero vector $v\in\mathbb R^n$,
    \begin{align*}
      \lambda_n \;=\;\max_{\|u\|_2=1}u^T A_n\,u
      \;\ge\;
      \max_{\|u\|_2=1} \sum_{i,j=1}^n A_n(i,j)u_i u_j\\
      \;\geq\;\sum_{i,j=1}^n A_n(i,j)\frac{1}{\sqrt{n}}\frac{1}{\sqrt{n}}
      \;=\;
      \frac1n\sum_{i,j}A_n(i,j) \to 1.
    \end{align*}
    For the other direction, we use part two. Choose $k$ such that $|x_k| = \max_{j} |x_j|$
    \begin{align*}
        \lambda_n \leq &\frac{x_k}{x_k} R_n(k) \to 1
    \end{align*}
\end{solution}
\begin{bbox}{(Straightforward) {2021 May Exam, \#7}}
    Suppose that $A=(a_{ij})_{1\le i,j \le 2}$ is a $2\times 2$ symmetric matrix, with $a_{11} = a_{22} =\frac{3}{4}$ and $a_{12} = a_{21} = \frac{1}{4}$. 
    \begin{itemize}
        \item Find the eigenvalues and eigenvectors of the matrix $A$.
        \item Compute $\lim_{n\to +\infty} a_{12}^{(n)}$ where $a_{i,j}^{(n)}$ denotes the $ij$th entry of the matrix $A^n$.
    \end{itemize}
\end{bbox}
\begin{solution}
    The first part is standard. Set up the characteristic polynomial and solve for its roots:
    \[
    p(\lambda) = \det(A-\lambda I) = 0 \implies \lambda = \frac{1}{2}, 1
    \]
    The eigenvector corresponding to $\lambda=1$ is $\begin{bmatrix}
         1/\sqrt{2}  \\
         1/\sqrt{2} 
    \end{bmatrix}$. The eigenvector corresponding to $\lambda=\frac{1}{2}$ is $\begin{bmatrix}
        1/\sqrt{2}\\ -1/\sqrt{2}
    \end{bmatrix}$.
    \newline
    For the second part. We should use diagonalization; otherwise, matrix exponential would be hard to compute.
    \[
    A = PDP^{-1}
    \]
    where $D$ is the diagonal matrix whose diagonal entries are the eigenvalues. $P^{-1}$ is the matrix whose the columns are the corresponding eigenvectors. So $D = \begin{bmatrix}
        1 & 0\\
        0 & \frac{1}{2}
    \end{bmatrix}$ and $ P^{-1} = \begin{bmatrix}
        1 & 1\\
        1 & -1
    \end{bmatrix}, \quad P= \begin{bmatrix}
        \frac{1}{2} & \frac{1}{2}\\
        \frac{1}{2} & -\frac{1}{2}
    \end{bmatrix}$. 
    \begin{align*}
    A^n &= P \begin{bmatrix}
        1^n & 0\\
        0 & \frac{1}{2}^n
    \end{bmatrix} P^{-1} = \begin{bmatrix}
        \frac{1}{2} & \frac{1}{2}\\
        \frac{1}{2} & -\frac{1}{2}
    \end{bmatrix} \begin{bmatrix}
        1 & 0\\
        0 & 0.5^n
    \end{bmatrix} \begin{bmatrix}
        1 & 1\\
        1 & -1
    \end{bmatrix}\\
    &= \begin{bmatrix}
        \frac{1}{2} & \frac{1}{2}\\
        \frac{1}{2} & -\frac{1}{2}
    \end{bmatrix} \begin{bmatrix}
        1 & 1\\
        0.5^n & -(0.5^n)
    \end{bmatrix}\\
    a_{12}^n = \frac{1}{2} - \frac{1}{2} \cdot (-(0.5^n)) \to \frac{1}{2}.
    \end{align*}
    This question is straightforward in my opinion!
\end{solution}
\begin{bbox}{{2021 Sept Exam, \#6}}
Let $A \in \mathbb R^{m\times n}$ be an $m\times n$ matrix with $n < m$. Suppose that $\lambda_1,\lambda_2,\dots, \lambda_n$ and $\vec v_1,\dots, \vec v_n$ are the eigenvalues and eigenvectors of $A^TA$. What can we say about ALL the eigenvalues and eigenvectors of $AA^T$. Justify your answer.
\end{bbox}
\begin{solution}
    When it comes $AA^T$, especially when $A$ is non-symmetric or even non-square, we should think of Singular Value Decomposition SVD! Let $A = U \Sigma V^{-1}$ be its SVD. Then $A^T = V \Sigma^{T} U^{-1}$.
    The singular values of $A$ are the square root of the eigenvalues of $AA^T$, and we see that $A$ and $A^T$ share the same singular values. Note that $U$ is composed of orthonormal eigenvectors of $AA^T$ and $V$ is composed of orthonormal eigenvectors of $A^TA$. $AA^T \vec v_i = \lambda \vec v_i $
\end{solution}
\begin{bbox}{Eigenvalue of Orthogonal Matrix}
    Let $A$ be a $3\times 3$ real-valued matrix such that $A^T A = AA^T = I_3$ and $\det (A) = 1$. Prove that $1$ is an eigenvalue of $A$.
\end{bbox}
\begin{solution}
    % Well, our first intuition should be that $1$ must a root of the characteristic polynomial. The problem wants to tell us that $A$ is orthogonal. The problem also told us something about the determinant, which kinda means that we should consider the characteristic polynomial. Okay, consider 
    % \[
    % \det (A - I) = 0 \iff \det(A^T(A-I))=0 \iff \det(I - A^T) =0
    % \]
    Since the problem wants to tell us that $A$ is orthogonal, we should be thinking of the length-preserving property. Let $\lambda$ be an eigenvalue of $A$ and $\vec v$ be a corresponding unit eigenvector. Then 
    \[
    \|A\vec v\| = \sqrt{\vec v^T A^T A \vec v} = 1 =\|\lambda \vec v\| = |\lambda|
    \]. 
    \newline
    The determinant is the product of the eigenvalues and $-1$ cannot be the only eigenvalue of $A$ because $(-1)^3 = -1 \neq 1 = \det A$.
\end{solution}
\begin{bbox}{(Straightforward) Trace of the square of a symmetric matrix is zero means zero matrix}
    Let $A$ be an $n\times n$ symmetric matrix such that $\Tr(A^2) = 0$. Show that $A = 0_{n\times n}$.
    \newline
    Hint: Use the fact that $\Tr(ABC) = \Tr(CAB)$.
\end{bbox}
\begin{solution}
    The hint apparently wants us to apply the spectral theorem to obtain a diagonalization $A = Q \Lambda Q^T$.
    \[
    \Tr A^2 = \Tr \left(Q\Lambda^2 Q^T\right) = \Tr\left(Q^T Q\Lambda^2\right) =\Tr(\Lambda^2)= 0.
    \]
    The trace is equal to the sum of the eigenvalues (to be honest, with this fact, we don't really need the hint), i.e. the diagonal of $\Lambda^2$ is zero. Since the entries of $\Lambda^2$ are non-zero, $\Lambda^2 = 0$ and hence $\Lambda = 0$. Therefore $A=0$.
\end{solution}

\begin{bbox}{Eigenvectors are the same iff Multiplication commutes}
    Let $A,B\in \mathbb R^{n\times n}$ have respective eigendecompositions $Q_1 D_1Q_1^T$ and $Q_2 D_2 Q_2^T$ (recall this means each $D_i$ is a diagonal matrix of eigenvalues and each $Q_i$ is an orthogonal matrix). Prove that $Q_1 = Q_2$ if and only if $AB = BA$. You may assume that $A, B$ do not have any repeated eigenvalues.
\end{bbox}
\begin{solution}
    Suppose $AB = BA$, consider an eigenpair $\lambda$ and $\vec v$ of $A$. 
    \[
    BA \vec v = \lambda B\vec v. = AB\vec v.
    \]
    This means that $B\vec v$ is an eigenvector of $A$ corresponding to the eigenvalue $\lambda$. This then imply to $A$ and $B$ share the same set of eigenvalues $\lambda_i$ with corresponding eigenvectors $\vec v_i$ and $B\vec v_i$. For $Q_1 = Q_2$, we need to show that $\vec v_i \propto B\vec v$: 
    \[
    A B\vec v = \lambda B\vec v, \implies B\vec v \propto \vec v
    \] since the eigenspaces of $A$ are all one-dimensional.
    \newline
    The other direction is easier. Suppose $Q_1 = Q_2$, then 
    \[
    AB = Q_1 D_1 Q_1^T Q_2 D_2 Q_2^T = Q_2 D_2 Q_2^TQ_1 D_1 Q_1^T = BA
    \]
\end{solution}
\begin{bbox}{(Straightforward) Eigenvalue of $uv^T$}
    Let $A = uv^T \in \mathbb R^{n\times n}$ be a rank-one matrix, i.e. $u, v \in \mathbb R^n$. Suppose $u,v \neq 0_n$. Find, with proof, all the eigenvalues of $A$.
\end{bbox}
\begin{solution}
    Let $\lambda$ be an eigenvalue of $A$ and $\vec x$ be a corresponding eigenvector, then 
    \[
    A \vec x= uv^T \vec x = \lambda \vec x 
    \]
    Note that 
    \[ 
    u v^T x = u \langle v, x\rangle = \lambda \vec x
    \]
    This means that $\vec x$ and $\vec u$ share the same direction. So 
    \[
    A \vec u = u v^T u = \lambda u
    \]
    Therefore, 
    \[
    \lambda = \vec v^T \vec u
    \]
    There can be no other eigenvalues because $A$ has rank-one.
    \newline
    Comment: Should find this problem straightforward.
\end{solution}
\begin{bbox}{Heisenberg Uncertainty Principle}
    Suppose $A, B \in \mathbb R^{n\times n}$ are symmetric matrices satisfying $AB + BA = Id$. Show that for all vectors $v\in \mathbb R^n\backslash\{0_n\}$,
    \[
    \max \{\frac{\|Av\|_2}{\|v\|_2}, \frac{\|Bv\|_2}{\|v\|_2}\} \geq 1/\sqrt{2}.
    \]
\end{bbox}
\begin{solution}
    This seems to be an interesting problem. Here's my thought process (which turned out to be misleading tho, because the canonical solution is simple): since the problem mentioned $A,B$ are symmetric and we are apparently dealing with Rayleigh quotient, and a maximization problem, so we should definitely look at the eigenvalue of $A,B$.
    \begin{enumerate}
        \item Since \(A\) and \(B\) are real symmetric, so is
            \[
            (A - B)^2.
            \]
            But for any symmetric \(X\),  
            \[
            X^2\;\succeq\;0
            \qquad\bigl(\text{i.e.\ }v^T X^2 v = \|Xv\|^2\ge0\bigr).
            \]
            Hence
            \[
            (A-B)^2 \;\succeq\;0
            \;\Longrightarrow\;
            A^2 - (AB+BA) + B^2 \;\succeq\;0
            \;\Longrightarrow\;
            A^2 + B^2 \;\succeq\;(AB+BA)\;=\;I.
            \]
            \item Loewner order \(A^2+B^2\succeq I\) means all eigenvalues of \(A^2+B^2\) are \(\ge 1\).  Equivalently, for every \(v\)
            \[
            v^T(A^2+B^2)\,v
            \;\ge\;v^T v
            \;\Longrightarrow\;
            \|Av\|^2 + \|Bv\|^2 \;\ge\;\|v\|^2.
            \]
            \item Finally,
                \[
                \max\{\|Av\|^2,\;\|Bv\|^2\}
                \;\ge\;\frac{\|Av\|^2 + \|Bv\|^2}{2}
                \;\ge\;\frac{\|v\|^2}{2},
                \]
                so
                \[
                \max\Bigl\{\frac{\|Av\|}{\|v\|},\,\frac{\|Bv\|}{\|v\|}\Bigr\}
                \;\ge\;\frac1{\sqrt 2},
                \]
    \end{enumerate}

    The canonical clean solution is actually to consider the inner product
    \begin{align*}
        \|v\|_2^2 =v^T v &= v^T I v = v^T(AB+BA)v\\
        &= v^T AB v + v^T BA v\\
        &\leq 2 |\langle Av, Bv \rangle| \\
        &\le 2 \|Av\|_2 \|Bv\|_2    
        \end{align*}
        where the last inequality is by Cauchy schwarz.
        \newline
        Finally, one of $\frac{\|Av\|_2}{\|v\|_2}$ and $\frac{\|Bv\|_2}{\|v\|_2}$ must be greater than $\frac{1}{\sqrt{2}}$. 
        \newline
        Comment: This teaches us a lesson that whenever we see the $2$-norm, consider playing with $v^Tv$. I was too obsessed with eigenvalues...
\end{solution} 
\begin{bbox}{Invertibility }
    Let $A=(a_{ij})$ be an $n\times n$ real matrix whose diagonal entries $a_{ii}$ satisfy $a_{ii}\ge 1$ for all $i$. Suppose also $\sum_{i\ne j} a_{ij} < 1$. Prove that the inverse matrix $A^{-1}$ exists.
\end{bbox}
\begin{solution}
    I gave myself the hint that I can use proof by contradiction since the statement is boolean.
    \begin{enumerate}
        \item Suppose by contradiction that $A$ is not invertible. Then the kernel of $A$ is non-trivial. (I did this step correctly). 
        \item We know that the diagonal entries are all greater than $1$ and the sum of all non-diagonal entries is less than $1$.
        \item This means that there exists $\vec v \ne \vec 0$ such that $A\vec v = \vec 0$ and assume that $\|\vec v\| = 1$. Then
        \begin{align*}
            &\sum_{j=1}^n a_{ij} v_j  =0\\
            &\implies a_{ii}v_{i} = -\sum_{j:j\ne i}a_{ij} v_j\\
            &\implies \sum_{i=1}^n a_{ii}^2 v_i^2 = \sum_{i=1}^n (\sum_{j:j\neq i}a_{ij}v_j)^2\\
        \end{align*}
        \item However, \[
        \sum_{i=1}^n a_{ii}^2 v_i^2\ge 1
        \]
        but 
        \begin{align*}
        &\sum_{i=1}^n(\sum_{j:j\ne i}a_{ij}v_j)^2 \le \sum_{i=1}^n \sum_{j:j\ne i}(a_{ij}^2) \sum_{j:j\ne i}(v_j)^2 = \sum_{i=1}^n \sum_{j: j\ne i} a_{ij}^2 < 1
        \end{align*}
    \end{enumerate}
\end{solution}
\begin{bbox}{Greshgorin Circle Problem}
    Let $A\in \mathbb C^{n\times n}$ with entries $a_{ij}$. Let \[
    R_i := \sum_{j\neq i} |a_{ij}|
    \]
    Let $D(a_{ii}, R_i)$ be a closed disc centered at $a_{ii}$ with radius $R_{i}$, called the Greshgorin disc. Show that every eigenvalue of $A$ lies at least in one of the Goshgorin discs of $A$.
    \end{bbox}
    \begin{solution}
        This appears to be a hard and interesting problem. My first intuition is proof by contradiction.
        \begin{enumerate}
            \item Suppose there exists $\lambda$ (with eigenvector $\vec v$) that doesn't lie in any Goshgorin disc: for all $i$
            \[
            |\lambda - a_{ii}| > R_i = \sum_{j\ne i} |a_{ij}|
            \]
            \item We need to reach a contradiction out of this. Let me try considering eigen vectors $\vec v$ such that the absolute value of the entries is upper bounded by $1$. Then 
            \begin{align*}
                &A v = \lambda v \\
                &\sum_{j=1}^n a_{ij}v_j=\lambda v_i\\
                &\sum_{j:j\ne i}a_{ij}v_j + a_{ii}v_i  = \lambda v_i\\
            \end{align*}
            Then we get 
            \[
            \sum_{j:j\neq i}|a_{ij}| \ge |\sum_{j:j\ne i}a_{ij}| = |v_i(\lambda-a_{ii})| = |\lambda-a_{ii}|
            \]
            which is a contradiction.
        \end{enumerate}
    \end{solution}
    
\end{document}